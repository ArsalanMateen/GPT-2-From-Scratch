{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25aa40e3-5109-433f-9153-f5770531fe94",
   "metadata": {},
   "source": [
    "# Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5d2c0-cba8-404e-9bf3-71a218cae3cf",
   "metadata": {},
   "source": [
    "Packages that are being used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.10.0\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a",
   "metadata": {},
   "source": [
    "This chapter covers data preparation and sampling to get input data \"ready\" for the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b2922-594d-4ff9-bd82-04f1ebdf41f5",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/01.webp?timestamp=1\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a769e87-470a-48b9-8bdb-12841b416198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 2470847\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/document.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2950a94-6b0d-474e-8ed0-66d0c3c1a95c",
   "metadata": {},
   "source": [
    "We can use the tokenizer to encode (that is, tokenize) texts into integers\n",
    "\n",
    "These integers can then be embedded (later) as input of/for the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21d347-ec03-4823-b3d4-9d686e495617",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/08.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336b43b-7173-49e7-bd80-527ad4efb271",
   "metadata": {},
   "source": [
    "We use the `<|endoftext|>` tokens between two independent sources of text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52442951-752c-4855-9752-b121a17fef55",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/10.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ba34b-170f-4e71-939b-77aabb776f14",
   "metadata": {},
   "source": [
    "## BytePair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309494c-79cf-4a2d-bc28-a94d602f050e",
   "metadata": {},
   "source": [
    "GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
    "\n",
    "It allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "\n",
    "We are using the BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library, which implements its core algorithms in Rust to improve computational performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2e7b4-6a22-42aa-8e4d-901f06378d4a",
   "metadata": {},
   "source": [
    "BPE tokenizers break down unknown words into subwords and individual characters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082d41f-33d7-4827-97d8-993d5a84bb3c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/11.webp\" width=\"300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd7c0d-70f8-4386-a114-907e96c950b0",
   "metadata": {},
   "source": [
    "## Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d9826-6384-462e-aa8a-a7c73cd6aad0",
   "metadata": {},
   "source": [
    "We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/12.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1770134-e7f3-4725-a679-e04c3be48cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/13.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ac652d-7b38-4843-9fbd-494cdc8ec12c",
   "metadata": {},
   "source": [
    "Create dataset and dataloader that extract chunks from the input text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b41073-4c9f-46e2-a1bd-d38e4122b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Insufficient tokens in input text for dataset creation\"\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb30ebe-97b3-43c5-9ff1-a97d621b3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9226d00c-ad9a-4949-a6e4-9afccfc7214f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[   54,  1219,   285,  ...,   885,   285, 10277],\n",
       "         [  333,   997, 11729,  ...,  2253,   502,   259],\n",
       "         [   72,   355,   430,  ...,   290,  3426,   737],\n",
       "         [ 7648,   885,  3338,  ...,  4352,   885,  3305]]),\n",
       " tensor([[ 1219,   285,  2724,  ...,   285, 10277,    72],\n",
       "         [  997, 11729,   256,  ...,   502,   259,  7648],\n",
       "         [  355,   430,   265,  ...,  3426,   737,   198],\n",
       "         [  885,  3338,   263,  ...,   885,  3305,   397]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=4, max_length=1024, stride=1024 // 2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10deb4bc-4de1-4d20-921e-4b1c7a0e1a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  198, 38485,    25,  ...,   256,   571,  8482],\n",
       "         [  397,  4563,   261,  ...,  5303,   350,  1077],\n",
       "         [45714,   573,    71,  ...,  7056,   502,   259],\n",
       "         [ 1462,  1976, 15498,  ...,   500,   318, 19918]]),\n",
       " tensor([[38485,    25, 13081,  ...,   571,  8482, 45714],\n",
       "         [ 4563,   261, 41727,  ...,   350,  1077,  1462],\n",
       "         [  573,    71,    80,  ...,   502,   259,   318],\n",
       "         [ 1976, 15498,   272,  ...,   318, 19918,    72]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/14.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ae6d45-f26e-4b83-9c7b-cff55ffa7d16",
   "metadata": {},
   "source": [
    "Note that we increase the stride so that we don't have overlaps between the batches, since more overlap could lead to increased overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1916e7a6-f03d-4f09-91a6-d0bdbac5a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   54,  1219,   285,  ...,   885,   285, 10277],\n",
      "        [  333,   997, 11729,  ...,  2253,   502,   259],\n",
      "        [   72,   355,   430,  ...,   290,  3426,   737],\n",
      "        [ 7648,   885,  3338,  ...,  4352,   885,  3305]])\n",
      "\n",
      "Targets:\n",
      " tensor([[ 1219,   285,  2724,  ...,   285, 10277,    72],\n",
      "        [  997, 11729,   256,  ...,   502,   259,  7648],\n",
      "        [  355,   430,   265,  ...,  3426,   737,   198],\n",
      "        [  885,  3338,   263,  ...,   885,  3305,   397]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=4, max_length=1024, stride=1024 // 2, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2fcda-2fda-4aa8-8bc8-de1e496f9db1",
   "metadata": {},
   "source": [
    "## Creating token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a301068-6ab2-44ff-a915-1ba11688274f",
   "metadata": {},
   "source": [
    "The data is already almost ready for an LLM\n",
    "\n",
    "But lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
    "\n",
    "Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85089aa-8671-4e5f-a2b3-ef252004ee4c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/15.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be97ced4-bd13-42b7-866a-4d699a17e155",
   "metadata": {},
   "source": [
    "\n",
    "An embedding layer is essentially a look-up operation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c2741-bf1b-4c60-b7fd-61409d556646",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/16.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c393d270-b950-4bc8-99ea-97d74f2ea0f6",
   "metadata": {},
   "source": [
    "## Encoding word positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24940068-1099-4698-bdc0-e798515e2902",
   "metadata": {},
   "source": [
    "Embedding layer convert IDs into identical vector representations regardless of where they are located in the input sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b14a2-f3f3-490e-b513-f262dbcf94fa",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/17.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7d7fe-38a5-46e6-8db6-b688887b0430",
   "metadata": {},
   "source": [
    "Positional embeddings are combined with the token embedding vector to form the input embeddings for a large language model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de37db-d54d-45c4-ab3e-88c0783ad2e4",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/18.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f187f87-c1f8-4c2e-8050-350bbb972f55",
   "metadata": {},
   "source": [
    "The BytePair encoder has a vocabulary size of 50,257:\n",
    "\n",
    "Suppose we want to encode the input tokens into a 768-dimensional vector representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b9e344d-03a6-4f2c-b723-67b6a20c5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 768\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2654722-24e4-4b0d-a43c-436a461eb70b",
   "metadata": {},
   "source": [
    "If we sample data from the dataloader, we embed the tokens in each batch into a 768-dimensional vector\n",
    "\n",
    "If we have a batch size of 4 with 1024 tokens each, this results in a 4 x 1024 x 768 tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad56a263-3d2e-4d91-98bf-d0b68d3c7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1024\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=4, max_length=max_length,\n",
    "    stride=max_length // 2 , shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84416b60-3707-4370-bcbc-da0b62f2b64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   54,  1219,   285,  ...,   885,   285, 10277],\n",
      "        [  333,   997, 11729,  ...,  2253,   502,   259],\n",
      "        [   72,   355,   430,  ...,   290,  3426,   737],\n",
      "        [ 7648,   885,  3338,  ...,  4352,   885,  3305]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([4, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7766ec38-30d0-4128-8c31-f49f063c43d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2001, -0.0106, -1.2923,  ...,  0.9828, -0.7837,  2.5149],\n",
       "         [-0.8918, -0.7310, -1.3799,  ..., -0.2804,  0.1272, -0.7561],\n",
       "         [-0.0820, -1.7687,  1.6987,  ...,  0.4435, -1.2183,  0.3573],\n",
       "         ...,\n",
       "         [ 1.0701, -0.4277,  1.3995,  ..., -0.5489,  1.1374, -2.0427],\n",
       "         [-0.0820, -1.7687,  1.6987,  ...,  0.4435, -1.2183,  0.3573],\n",
       "         [-0.9788,  1.5289, -0.8870,  ..., -0.8148,  0.0560,  0.6873]],\n",
       "\n",
       "        [[ 0.6641, -0.1710, -0.8509,  ...,  0.0109, -2.0909,  0.7179],\n",
       "         [-0.2338,  0.1532,  0.5148,  ..., -1.8632,  1.1932,  1.4671],\n",
       "         [-0.1500,  0.3272, -0.4963,  ..., -1.2302,  1.7294,  0.0213],\n",
       "         ...,\n",
       "         [-1.3542, -0.1369, -1.1837,  ...,  0.5031, -0.0115, -0.1423],\n",
       "         [-2.2588,  1.9325, -0.4783,  ...,  2.2393, -0.6354, -0.9153],\n",
       "         [-1.2025, -0.3443, -1.5586,  ..., -0.3419, -0.4331, -0.3141]],\n",
       "\n",
       "        [[-0.5525, -0.4649,  0.7483,  ...,  0.1353,  0.0983, -1.3722],\n",
       "         [ 0.6163,  0.3344, -0.0321,  ...,  1.1629,  0.3358,  1.0613],\n",
       "         [-0.6878,  0.1300, -0.1958,  ..., -1.1769, -1.0380, -0.0641],\n",
       "         ...,\n",
       "         [-0.7420,  1.6637, -0.3797,  ..., -0.8655, -0.0588,  0.7437],\n",
       "         [ 0.1967, -1.3589, -0.4791,  ...,  2.2005, -0.9574, -0.1507],\n",
       "         [-0.5190, -0.3866, -0.2411,  ..., -0.1040, -0.0431,  0.4138]],\n",
       "\n",
       "        [[-0.6222,  0.8022, -0.1992,  ...,  0.8330,  1.7112, -0.6769],\n",
       "         [ 1.0701, -0.4277,  1.3995,  ..., -0.5489,  1.1374, -2.0427],\n",
       "         [-0.1053,  0.2491, -0.4093,  ..., -0.1569,  0.0816,  0.4598],\n",
       "         ...,\n",
       "         [-0.0572, -0.4487,  0.0718,  ...,  0.0242,  1.7177, -2.0438],\n",
       "         [ 1.0701, -0.4277,  1.3995,  ..., -0.5489,  1.1374, -2.0427],\n",
       "         [-0.4694, -0.7146,  0.1837,  ...,  1.5442, -1.1214, -0.9971]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)\n",
    "\n",
    "(token_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2ae164-6f19-4e32-b9e5-76950fcf1c9f",
   "metadata": {},
   "source": [
    "GPT-2 uses absolute position embeddings, so we just create another embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc048e20-7ac8-417e-81f5-8fe6f9a4fe07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2960,  0.5421, -0.6717,  ..., -1.0487,  0.3583, -0.0158],\n",
       "        [-0.8249,  2.6008,  0.2817,  ..., -0.4738,  0.2014, -1.5513],\n",
       "        [-1.5623,  0.3674,  1.0243,  ...,  1.0584, -1.8554, -0.8419],\n",
       "        ...,\n",
       "        [ 0.5705,  1.0253, -1.8635,  ...,  0.6571,  0.2628,  0.5900],\n",
       "        [-0.4751, -1.3380,  0.2011,  ...,  1.2281, -1.1136,  0.1384],\n",
       "        [ 1.1393, -1.8344,  1.5052,  ..., -0.4528,  0.9091,  1.6983]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c369a1e7-d566-4b53-b398-d6adafb44105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2960,  0.5421, -0.6717,  ..., -1.0487,  0.3583, -0.0158],\n",
       "        [-0.8249,  2.6008,  0.2817,  ..., -0.4738,  0.2014, -1.5513],\n",
       "        [-1.5623,  0.3674,  1.0243,  ...,  1.0584, -1.8554, -0.8419],\n",
       "        ...,\n",
       "        [ 0.5705,  1.0253, -1.8635,  ...,  0.6571,  0.2628,  0.5900],\n",
       "        [-0.4751, -1.3380,  0.2011,  ...,  1.2281, -1.1136,  0.1384],\n",
       "        [ 1.1393, -1.8344,  1.5052,  ..., -0.4528,  0.9091,  1.6983]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "(pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870e9d9f-2935-461a-9518-6d1386b976d6",
   "metadata": {},
   "source": [
    "To create the input embeddings used in an LLM, we simply add the token and the positional embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b22fab89-526e-43c8-9035-5b7018e34288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4961,  0.5315, -1.9641,  ..., -0.0659, -0.4254,  2.4990],\n",
       "         [-1.7167,  1.8699, -1.0983,  ..., -0.7542,  0.3286, -2.3073],\n",
       "         [-1.6443, -1.4013,  2.7230,  ...,  1.5019, -3.0737, -0.4847],\n",
       "         ...,\n",
       "         [ 1.6406,  0.5976, -0.4640,  ...,  0.1082,  1.4002, -1.4527],\n",
       "         [-0.5572, -3.1067,  1.8997,  ...,  1.6716, -2.3320,  0.4957],\n",
       "         [ 0.1605, -0.3055,  0.6183,  ..., -1.2676,  0.9651,  2.3856]],\n",
       "\n",
       "        [[ 0.3681,  0.3711, -1.5226,  ..., -1.0378, -1.7326,  0.7021],\n",
       "         [-1.0587,  2.7541,  0.7965,  ..., -2.3370,  1.3946, -0.0842],\n",
       "         [-1.7123,  0.6946,  0.5280,  ..., -0.1718, -0.1260, -0.8206],\n",
       "         ...,\n",
       "         [-0.7837,  0.8884, -3.0473,  ...,  1.1603,  0.2513,  0.4477],\n",
       "         [-2.7339,  0.5945, -0.2772,  ...,  3.4674, -1.7491, -0.7769],\n",
       "         [-0.0632, -2.1787, -0.0533,  ..., -0.7947,  0.4760,  1.3842]],\n",
       "\n",
       "        [[-0.8486,  0.0772,  0.0765,  ..., -0.9134,  0.4565, -1.3880],\n",
       "         [-0.2086,  2.9353,  0.2496,  ...,  0.6891,  0.5372, -0.4900],\n",
       "         [-2.2501,  0.4974,  0.8285,  ..., -0.1185, -2.8934, -0.9061],\n",
       "         ...,\n",
       "         [-0.1716,  2.6890, -2.2432,  ..., -0.2084,  0.2041,  1.3337],\n",
       "         [-0.2784, -2.6969, -0.2780,  ...,  3.4286, -2.0711, -0.0123],\n",
       "         [ 0.6202, -2.2210,  1.2641,  ..., -0.5568,  0.8660,  2.1121]],\n",
       "\n",
       "        [[-0.9182,  1.3443, -0.8709,  ..., -0.2157,  2.0695, -0.6928],\n",
       "         [ 0.2452,  2.1731,  1.6812,  ..., -1.0227,  1.3387, -3.5939],\n",
       "         [-1.6676,  0.6164,  0.6150,  ...,  0.9015, -1.7738, -0.3822],\n",
       "         ...,\n",
       "         [ 0.5132,  0.5766, -1.7918,  ...,  0.6814,  1.9805, -1.4539],\n",
       "         [ 0.5950, -1.7657,  1.6006,  ...,  0.6792,  0.0237, -1.9042],\n",
       "         [ 0.6699, -2.5490,  1.6890,  ...,  1.0914, -0.2122,  0.7012]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "(input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bb0f7e-460d-44db-b366-096adcd84fff",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch02_compressed/19.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9515017-c6ab-4b64-8afb-618b6d4b3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    input_ids, target_ids = batch\n",
    "\n",
    "    token_embeddings = token_embedding_layer(input_ids)\n",
    "    pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "\n",
    "    input_embeddings = token_embeddings + pos_embeddings\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b9969b0-1ef7-426c-b839-961f0c1a1c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "print(input_embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
